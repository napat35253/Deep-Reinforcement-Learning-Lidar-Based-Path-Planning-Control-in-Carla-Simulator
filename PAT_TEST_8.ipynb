{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORT DONE\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.applications.xception import Xception\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "import pandas as pd\n",
    "from plotnine import *\n",
    "#from keras.callbacks import TensorBoard\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as backend\n",
    "from threading import Thread\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "try:\n",
    "    sys.path.append(glob.glob('../carla/dist/carla-*%d.%d-%s.egg' % (\n",
    "        sys.version_info.major,\n",
    "        sys.version_info.minor,\n",
    "        'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])\n",
    "except IndexError:\n",
    "    pass\n",
    "import carla\n",
    "print('IMPORT DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Car Environment [Action/Reward here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CarEnv:\n",
    "    #BRAKE_AMT = 1.0\n",
    "    actor_list = []\n",
    "    collision_hist = []\n",
    "    pt_cloud = []\n",
    "    pt_cloud_filtered = []\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = carla.Client('localhost', 2000)\n",
    "        self.client.set_timeout(2.0)\n",
    "        self.world = self.client.get_world()\n",
    "        blueprint_library = self.world.get_blueprint_library()\n",
    "        self.model_3 = blueprint_library.filter('model3')[0]\n",
    "        self.truck_2 = blueprint_library.filter('carlamotors')[0]\n",
    "        \n",
    "    def Black_screen(self):\n",
    "        settings = self.world.get_settings()\n",
    "        settings.no_rendering_mode = True\n",
    "        self.world.apply_settings(settings)\n",
    "                     \n",
    "    def reset(self):\n",
    "        self.collision_hist = []\n",
    "        self.actor_list = []\n",
    "        self.pt_cloud = []\n",
    "        self.pt_cloud_filtered = []\n",
    "        place=random.uniform(110,150)\n",
    "        ##print('Location: ',str(place))\n",
    "        #transform = carla.Transform(carla.Location(-120,place,3),carla.Rotation(0,-90,0))\n",
    "        transform = carla.Transform(carla.Location(246,-36,3),carla.Rotation(0,-90,0))        \n",
    "        self.flag = 0\n",
    "        self.vehicle = self.world.spawn_actor(self.model_3, transform)\n",
    "        self.flag = 1\n",
    "        \n",
    "        self.actor_list.append(self.vehicle)\n",
    "     \n",
    "\n",
    "        self.lidar_sensor = self.world.get_blueprint_library().find('sensor.lidar.ray_cast')\n",
    "        self.lidar_sensor.set_attribute('points_per_second', '100000')\n",
    "        self.lidar_sensor.set_attribute('channels', '32')\n",
    "        self.lidar_sensor.set_attribute('range', '10000')\n",
    "        self.lidar_sensor.set_attribute('upper_fov', '10')\n",
    "        self.lidar_sensor.set_attribute('lower_fov', '-10')\n",
    "        self.lidar_sensor.set_attribute('rotation_frequency', '60')\n",
    "        \n",
    "        transform = carla.Transform(carla.Location(x=0, z=1.9))\n",
    "        time.sleep(0.01)\n",
    "\n",
    "        self.sensor = self.world.spawn_actor(self.lidar_sensor, transform, attach_to=self.vehicle)\n",
    "     \n",
    "        self.actor_list.append(self.sensor)\n",
    "        self.sensor.listen(lambda data: self.process_lidar(data))\n",
    "\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=1, brake=0.0))\n",
    "        self.episode_start = time.time()\n",
    "   \n",
    "        time.sleep(0.4) # sleep to get things started and to not detect a collision when the car spawns/falls from sky.\n",
    "        \n",
    "        transform2 = carla.Transform(carla.Location(x=2.5, z=0.7))\n",
    "        colsensor = self.world.get_blueprint_library().find('sensor.other.collision')\n",
    "        self.colsensor = self.world.spawn_actor(colsensor, transform2, attach_to=self.vehicle)\n",
    "        self.actor_list.append(self.colsensor)\n",
    "        self.colsensor.listen(lambda event: self.collision_data(event))\n",
    "\n",
    "        while self.distance_to_obstacle_f is None:\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        self.episode_start = time.time()\n",
    "        \n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=1, brake=0.0))\n",
    "        \n",
    "        ##SENSOR LIDAR\n",
    "        xx = self.distance_to_obstacle_f\n",
    "        yy = self.distance_to_obstacle_r\n",
    "        zz = self.distance_to_obstacle_l\n",
    "        state_=np.array([xx,yy,zz])\n",
    "        \n",
    "        return state_\n",
    "\n",
    "    def collision_data(self, event):\n",
    "        self.collision_hist.append(event)\n",
    "\n",
    "    def process_lidar(self, raw):\n",
    "        points = np.frombuffer(raw.raw_data, dtype=np.dtype('f4'))\n",
    "        points = np.reshape(points, (int(points.shape[0] / 3), 3))*np.array([1,-1,-1])\n",
    "        \n",
    "        lidar_f = lidar_line(points,90,2)\n",
    "        lidar_r = lidar_line(points,45,2)\n",
    "        lidar_l = lidar_line(points,135,2)\n",
    "\n",
    "        if len(lidar_f) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            self.distance_to_obstacle_f = min(lidar_f[:,1])-2.247148275375366\n",
    "        \n",
    "        if len(lidar_r) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            self.distance_to_obstacle_r = np.sqrt(min(lidar_r[:,0]**2 + lidar_r[:,1]**2))\n",
    "        \n",
    "        if len(lidar_l) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            self.distance_to_obstacle_l = np.sqrt(min(lidar_l[:,0]**2 + lidar_l[:,1]**2))\n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        sleepy=0.2\n",
    "        \n",
    "        if action == 0:\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, brake=0.0, steer = 0.3))\n",
    "            time.sleep(sleepy)\n",
    "            reward = 0.5\n",
    "        elif action == 1:\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, brake=0.0, steer = -0.3))\n",
    "            time.sleep(sleepy)\n",
    "            reward = 0.5\n",
    "        elif action == 2 :\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, brake=0.0, steer = 0.0))\n",
    "            time.sleep(sleepy)\n",
    "            reward = 2  \n",
    "        elif action == 3 :\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, brake=0.0, steer = 0.1))\n",
    "            time.sleep(sleepy)\n",
    "            reward = 1  \n",
    "        elif action == 4 :\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, brake=0.0, steer = -0.1))\n",
    "            time.sleep(sleepy)\n",
    "            reward = 1  \n",
    "\n",
    "        \n",
    "        if len(self.collision_hist) != 0:\n",
    "            done = True\n",
    "            reward = -10\n",
    "        else :\n",
    "            done=False\n",
    "            \n",
    "        if self.episode_start + SECONDS_PER_EPISODE < time.time():\n",
    "            done = True\n",
    "            \n",
    "        xx = self.distance_to_obstacle_f\n",
    "        yy = self.distance_to_obstacle_r\n",
    "        zz = self.distance_to_obstacle_l\n",
    "        state_=np.array([xx,yy,zz])\n",
    "            \n",
    "        return state_, reward, done, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Optional Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def open_carla(require):\n",
    "    try:\n",
    "        if require == 'fast':\n",
    "            os.popen('C:\\\\Users\\\\nbhah\\\\Desktop\\\\Carla\\\\CarlaUE4\\\\Binaries\\\\Win64\\\\CarlaUE4.exe -benchmark  -fps=10 -quality-level=Low')\n",
    "        else:\n",
    "            os.popen('C:\\\\Users\\\\nbhah\\\\Desktop\\\\Carla\\\\CarlaUE4\\\\Binaries\\\\Win64\\\\CarlaUE4.exe')\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "    print('opening Carla')\n",
    "    \n",
    "def close_carla():\n",
    "    try:\n",
    "        os.system('TASKKILL /F /IM CarlaUE4.exe')\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "def xxx():\n",
    "    env.world.wait_for_tick()\n",
    "    for x in list(env.world.get_actors()):\n",
    "        if x.type_id == 'vehicle.tesla.model3' or x.type_id == 'sensor.lidar.ray_cast' or x.type_id == 'sensor.other.collision':\n",
    "            x.destroy()\n",
    "            \n",
    "def lidar_line(points,degree,width):\n",
    "    angle = degree*(2*np.pi)/360\n",
    "    points_l = points\n",
    "    points_l = points_l[np.logical_and(points_l[:,2] > -1.75, points_l[:,2] < 1000)] #z\n",
    "    points_l = points_l[np.logical_and(np.tan(angle)*points_l[:,0]+width*np.sqrt(1+np.tan(angle)**2)>=points_l[:,1], np.tan(angle)*points_l[:,0]-width*np.sqrt(1+np.tan(angle)**2)<=points_l[:,1])] #y\n",
    "    if 180>degree >0:\n",
    "        points_l = points_l[np.logical_and(points_l[:,1]>0, points_l[:,1]<1000)] #y>0\n",
    "    if 180<degree<360:\n",
    "        points_l = points_l[np.logical_and(points_l[:,1]<0, points_l[:,1] > -1000)] #x\n",
    "    if degree == 0 or degree == 360:\n",
    "        points_l = points_l[np.logical_and(points_l[:,0]>0,points_l[:,0] <1000 )] #x\n",
    "    if degree == 180:\n",
    "        points_l = points_l[np.logical_and(points_l[:,0] >-1000 , points_l[:,0]<0 )]\n",
    "    return  points_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self,state_size,action_size):\n",
    "        self.state_size=state_size\n",
    "        self.action_size=action_size\n",
    "        self.model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "\n",
    "        self.target_update_counter = 0\n",
    "        #self.graph = tf.get_default_graph()\n",
    "\n",
    "        self.terminate = False\n",
    "        self.last_logged_episode = 0\n",
    "        self.training_initialized = False\n",
    "    \n",
    "    def get_weight(self):\n",
    "        \n",
    "        w = self.model.get_weights()        \n",
    "        return w\n",
    "    \n",
    "    def predict(self,state):\n",
    "        \n",
    "        predict = self.model.predict(state.reshape((1, self.state_size)))\n",
    "        return predict\n",
    "    \n",
    "    def save_model(self,name):\n",
    "        model_json = self.model.to_json()\n",
    "        with open(\"{}.json\".format(name), \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        self.model.save_weights(\"{}.h5\".format(name))\n",
    "        print(\"Saved model to disk\")     \n",
    "        \n",
    "\n",
    "    def create_model(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(64, input_dim=32, activation='relu')) \n",
    "        model.add(Dense(self.action_size, activation='softmax'))            # output nodes = #action\n",
    "        model.build()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "        \n",
    "        return model\n",
    "    def loss(self,y_test,y_pred,action):\n",
    "        \n",
    "        self.compute_loss = sparse_categorical_crossentropy(y_test,y_pred,from_logits=True)        \n",
    "        self.loss=self.compute_loss([action],tf.math.log(self.model))\n",
    "\n",
    "    def update_replay_memory(self, transition):\n",
    "        # transition = (current_state, action, reward, new_state, done)\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    def train(self):\n",
    "        global Loss\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            self.terminate=True\n",
    "            Loss.append(0)\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "        \n",
    "        current_states = np.array([transition[0] for transition in minibatch])\n",
    "        \n",
    "        #with self.graph.as_default():\n",
    "        current_qs_list = self.model.predict(current_states, PREDICTION_BATCH_SIZE)\n",
    "\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])\n",
    "        \n",
    "        #with self.graph.as_default():\n",
    "        future_qs_list = self.target_model.predict(new_current_states, PREDICTION_BATCH_SIZE)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for index, (current_state, action, reward, new_state, done) in enumerate(minibatch):\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "            \n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "\n",
    "        history=self.model.fit(np.array(X), np.array(y), batch_size=TRAINING_BATCH_SIZE, verbose=0, shuffle=False)\n",
    "        Loss.append(history.history['loss'][0])\n",
    "\n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(state.reshape((1, self.state_size)))[0]\n",
    "        \n",
    "    def train_in_loop(self):\n",
    "        X = np.random.uniform(size=(1, self.state_size)).astype(np.float32)\n",
    "        y = np.random.uniform(size=(1, self.action_size)).astype(np.float32)\n",
    "        \n",
    "        self.model.fit(X,y, verbose=False, batch_size=1)\n",
    "\n",
    "        self.training_initialized = True\n",
    "        print('Start Train')\n",
    "        while True:\n",
    "            if self.terminate:\n",
    "                return\n",
    "            self.train()\n",
    "            time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(32, input_dim = 4, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(2, activation = \"softmax\"))\n",
    "model.build()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "compute_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.3708975e-05, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "s=np.array([10,10,10,10])\n",
    "s = s.reshape([1,4])\n",
    "with tf.GradientTape() as tape:\n",
    "    logits = model(s)\n",
    "    a_dist = logits.numpy()\n",
    "    a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "    a = np.argmax(a_dist == a)\n",
    "    loss = compute_loss([a], tf.math.log(logits))\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECONDS_PER_EPISODE = 100\n",
    "REPLAY_MEMORY_SIZE = 5_000\n",
    "MIN_REPLAY_MEMORY_SIZE = 32\n",
    "MINIBATCH_SIZE = 32\n",
    "PREDICTION_BATCH_SIZE = 1\n",
    "TRAINING_BATCH_SIZE = MINIBATCH_SIZE // 4\n",
    "UPDATE_TARGET_EVERY = 5\n",
    "\n",
    "\n",
    "MEMORY_FRACTION = 0.4\n",
    "MIN_REWARD = -200\n",
    "\n",
    "EPISODES = 1000\n",
    "\n",
    "DISCOUNT = 0.99\n",
    "epsilon = 1\n",
    "EPSILON_DECAY = 0.99975 ## 0.9975 99975\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "AGGREGATE_STATS_EVERY = 10\n",
    "state_size=3\n",
    "action_size=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    close_carla()    \n",
    "    open_carla('fast')\n",
    "    time.sleep(15)\n",
    "    FPS=60\n",
    "\n",
    "    ep_rewards = []\n",
    "    ep=[]\n",
    "    Step=[]\n",
    "    Loss=[]\n",
    "    random.seed(1)\n",
    "    np.random.seed(1)\n",
    "   \n",
    "    # Create agent and environment\n",
    "    agent = DQNAgent(state_size,action_size)\n",
    "    env = CarEnv()\n",
    "    env.Black_screen()\n",
    "    xxx()\n",
    "    \n",
    "    agent.train_in_loop()\n",
    "    agent.get_qs(np.ones((1, state_size)))\n",
    "\n",
    "    # Iterate over episodes\n",
    "    with tqdm(total=EPISODES) as pbar:\n",
    "        \n",
    "        for episode in range(EPISODES):\n",
    "            \n",
    "            #try:\n",
    "\n",
    "            env.collision_hist = []\n",
    "            episode_reward = 0\n",
    "            loss=0\n",
    "            step = 1\n",
    "\n",
    "            # Reset environment and get initial state\n",
    "            current_state = env.reset()\n",
    "\n",
    "            # Reset flag and start iterating until episode ends\n",
    "            done = False\n",
    "            episode_start = time.time()\n",
    "\n",
    "            # Play for given number of seconds only\n",
    "            while True:\n",
    "                \n",
    "                with tf.GradientTape() as tape:\n",
    "\n",
    "                # This part stays mostly the same, the change is to query a model for Q values\n",
    "                if np.random.random() > epsilon:\n",
    "                    # Get action from Q table\n",
    "                    action = np.argmax(agent.get_qs(current_state))\n",
    "                else:\n",
    "                    # Get random action\n",
    "                    action = np.random.randint(0, action_size)\n",
    "                    # This takes no time, so we add a delay matching 60 FPS (prediction above takes longer)\n",
    "                    time.sleep(1/FPS)\n",
    "\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                # Transform new continous state to new discrete state and count reward\n",
    "                episode_reward += reward\n",
    "             \n",
    "                # Every step we update replay memory\n",
    "                agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "                current_state = new_state\n",
    "                step += 1\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            # End of episode - destroy agents\n",
    "            for actor in env.actor_list:\n",
    "                actor.destroy()\n",
    "            agent.train()\n",
    "            \n",
    "            # Decay epsilon\n",
    "            if epsilon > MIN_EPSILON:\n",
    "                epsilon *= EPSILON_DECAY\n",
    "                epsilon = max(MIN_EPSILON, epsilon)\n",
    "                \n",
    "            print('Episode :{}, Step :{}, Epsilon :{} ,Reward :{}'\\\n",
    "                  .format(episode,step,epsilon,episode_reward))\n",
    "            \n",
    "            ep_rewards.append(episode_reward)\n",
    "            ep.append(episode)\n",
    "            Step.append(step)\n",
    "            \n",
    "            pbar.update(1)\n",
    "    close_carla()       \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Episode':ep,'Reward':ep_rewards,'Step':Step,'Loss':Loss[1:]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ggplot(df,aes(x='Reward'))+\\\n",
    "geom_histogram(binwidth=0.5,fill='#48b6a3')+\\\n",
    "xlim(-10,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ggplot(df, aes(x='Episode',y='Reward'))+ \\\n",
    "  geom_point(color=\"#F18F01\", size=0.2)+\\\n",
    "    geom_smooth(method=\"lm\", se=False, size=0.5, linetype=\"dashed\")+\\\n",
    "        ylim(-10,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ggplot(df, aes(x='Episode',y='Step'))+ \\\n",
    "  geom_point(color=\"#F18F01\", size=0.2)+\\\n",
    "    geom_smooth(method=\"lm\", se=False, size=0.5, linetype=\"dashed\")+\\\n",
    "        ylim(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ggplot(df, aes(x='Episode',y='Loss'))+ \\\n",
    "  geom_point(color=\"#F18F01\", size=0.2)+\\\n",
    "    geom_smooth(method=\"lm\", se=False, size=0.5, linetype=\"dashed\")+\\\n",
    "        ylim(0,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Save data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "name='Test' ##INSERT FILE NAME\n",
    "file_path=\"DATA\\\\ \"\n",
    "df.to_csv(file_path+'{}.csv'.format(name))\n",
    "agent.save_model(file_path+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name='Test'\n",
    "\n",
    "json_file = open('DATA\\\\ {}.json'.format(model_name), 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"DATA\\\\{}.h5\".format(model_name))\n",
    "print(\"Loaded model from disk\")\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open_carla('not fast')\n",
    "#time.sleep(15)\n",
    "\n",
    "ep=10\n",
    "test_step=[]\n",
    "test_reward=[]\n",
    "\n",
    "env=CarEnv()\n",
    "\n",
    "with tqdm(total=EPISODES) as pbar:\n",
    "    for episode in range(ep):\n",
    "        state=env.reset()\n",
    "        done = False\n",
    "        step=0\n",
    "        rewards=0\n",
    "        while True:\n",
    "            step+=1\n",
    "            state=np.array([[i for i in state]])\n",
    "            a=loaded_model.predict(state)\n",
    "            action=a.argmax()\n",
    "            time.sleep(0.01)\n",
    "            state,reward,done,_=env.step(action)\n",
    "            rewards+=reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        for actor in env.actor_list:\n",
    "                actor.destroy()\n",
    "                \n",
    "        test_reward.append(rewards)\n",
    "        test_step.append(step)\n",
    "        pbar.update(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.DataFrame({'Episode':[i for i in range(len(test_step))],\\\n",
    "                      'Step':test_step,'Reward':test_reward})\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(df_test, aes(x='Episode',y='Reward'))+ \\\n",
    "  geom_point(color=\"#F18F01\", size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_carla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "KWY Contents",
   "title_sidebar": "KWY Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
