{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import smrclib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sys.path.append(glob.glob('../carla/dist/carla-*%d.%d-%s.egg' % (\n",
    "        sys.version_info.major,\n",
    "        sys.version_info.minor,\n",
    "        'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])\n",
    "except IndexError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "SECONDS_PER_EPISODE = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xxx():\n",
    "    for actor in env.actor_list:\n",
    "        actor.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarEnv:\n",
    "    #BRAKE_AMT = 1.0\n",
    "\n",
    "    actor_list = []\n",
    "    collision_hist = []\n",
    "\n",
    "    pt_cloud = []\n",
    "    pt_cloud_filtered = []\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = carla.Client('localhost', 2000)\n",
    "        self.client.set_timeout(2.0)\n",
    "\n",
    "        self.world = self.client.get_world()\n",
    "\n",
    "        blueprint_library = self.world.get_blueprint_library()\n",
    "\n",
    "        self.model_3 = blueprint_library.filter('model3')[0]\n",
    "        self.truck_2 = blueprint_library.filter('carlamotors')[0]\n",
    "        \n",
    "    def reset(self):\n",
    "        self.collision_hist = []\n",
    "        self.actor_list = []\n",
    "        self.pt_cloud = []\n",
    "        self.pt_cloud_filtered = []\n",
    "\n",
    "        transform = carla.Transform(carla.Location(-120,115,3),carla.Rotation(0,-90,0))\n",
    "\n",
    "        self.vehicle = self.world.spawn_actor(self.model_3, transform)\n",
    "        \n",
    "        self.actor_list.append(self.vehicle)\n",
    "\n",
    "        self.lidar_sensor = self.world.get_blueprint_library().find('sensor.lidar.ray_cast')\n",
    "        self.lidar_sensor.set_attribute('points_per_second', '100000')\n",
    "        self.lidar_sensor.set_attribute('channels', '32')\n",
    "        self.lidar_sensor.set_attribute('range', '10000')\n",
    "        self.lidar_sensor.set_attribute('upper_fov', '10')\n",
    "        self.lidar_sensor.set_attribute('lower_fov', '-10')\n",
    "        self.lidar_sensor.set_attribute('rotation_frequency', '60')\n",
    "\n",
    "        transform = carla.Transform(carla.Location(x=0, z=1.9))\n",
    "        self.sensor = self.world.spawn_actor(self.lidar_sensor, transform, attach_to=self.vehicle)\n",
    "\n",
    "        self.actor_list.append(self.sensor)\n",
    "        self.sensor.listen(lambda data: self.process_lidar(data))\n",
    "\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=1, brake=0.0))\n",
    "        self.episode_start = time.time()\n",
    "\n",
    "\n",
    "        time.sleep(4) # sleep to get things started and to not detect a collision when the car spawns/falls from sky.\n",
    "        \n",
    "        transform2 = carla.Transform(carla.Location(x=2.5, z=0.7))\n",
    "        colsensor = self.world.get_blueprint_library().find('sensor.other.collision')\n",
    "        self.colsensor = self.world.spawn_actor(colsensor, transform2, attach_to=self.vehicle)\n",
    "        self.actor_list.append(self.colsensor)\n",
    "        self.colsensor.listen(lambda event: self.collision_data(event))\n",
    "\n",
    "        while self.distance_to_obstacle is None:\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        self.episode_start = time.time()\n",
    "\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=1, brake=0.0))\n",
    "        \n",
    "        return int(self.distance_to_obstacle//1)\n",
    "\n",
    "    def collision_data(self, event):\n",
    "        self.collision_hist.append(event)\n",
    "\n",
    "    def process_lidar(self, raw):\n",
    "        points = np.frombuffer(raw.raw_data, dtype=np.dtype('f4'))\n",
    "        points = np.reshape(points, (int(points.shape[0] / 3), 3))*np.array([1,-1,-1])\n",
    "        lidar_data = points.astype(np.int32)\n",
    "        self.pt_cloud.append(lidar_data)\n",
    "\n",
    "        #screen points specifically -4<y<4 and 0<x<12\n",
    "        pt = points[np.logical_and(points[:,0] > -3, points[:,0] < 3)]\n",
    "        points_filter = pt[np.logical_and(pt[:,1] > 0, pt[:,1] < 50)]\n",
    "        points_filter = points_filter[np.logical_and(points_filter[:,1] > 0, points_filter[:,1] < 50)]\n",
    "        self.pt_cloud_filtered.append(points_filter)\n",
    "\n",
    "        if len(points_filter) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            self.distance_to_obstacle = min(points_filter[:,1])-2.247148275375366\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        v = self.vehicle.get_velocity()\n",
    "        kmh = int(3.6 * math.sqrt(v.x**2 + v.y**2 + v.z**2))\n",
    "        \n",
    "        if action == 0:\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, brake=0.0, steer = 0.0))\n",
    "        elif action == 1:\n",
    "            while kmh != 0:\n",
    "                self.vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=1.0, steer = 0.0))\n",
    "                ##print(kmh)\n",
    "                v = self.vehicle.get_velocity()\n",
    "                kmh = int(3.6 * math.sqrt(v.x**2 + v.y**2 + v.z**2))\n",
    "                ##print(\"distance_to_obstacle = \",self.distance_to_obstacle)\n",
    "                \n",
    "        # ถ้ารถชน หรือ รถหยุดแล้ว\n",
    "        if kmh == 0 or len(self.collision_hist) != 0 :\n",
    "            done = True\n",
    "            print('DISTANCE FOR REWARD')\n",
    "            print(self.distance_to_obstacle)\n",
    "            if  0<= self.distance_to_obstacle <1 and len(self.collision_hist) != 0 :\n",
    "                reward = -1\n",
    "            elif  0<= self.distance_to_obstacle <1:\n",
    "                reward = 1\n",
    "            elif  1<= self.distance_to_obstacle <2:\n",
    "                reward = 1.5\n",
    "            elif  2<= self.distance_to_obstacle <3:\n",
    "                reward = 0.45\n",
    "            elif  3<= self.distance_to_obstacle <4:\n",
    "                reward = 0.32\n",
    "            elif  4<= self.distance_to_obstacle <5:\n",
    "                reward = 0.20\n",
    "            elif  5<= self.distance_to_obstacle <6:\n",
    "                reward = -0.15\n",
    "            elif  6<= self.distance_to_obstacle <7:\n",
    "                reward = -0.8\n",
    "            elif  7<= self.distance_to_obstacle <8:\n",
    "                reward = -1.5\n",
    "            elif  8<= self.distance_to_obstacle :\n",
    "                reward = -3\n",
    "        else:\n",
    "            done = False\n",
    "            reward = 0.5\n",
    "        \n",
    "        if self.episode_start + SECONDS_PER_EPISODE < time.time():\n",
    "            done = True\n",
    "            reward = -1\n",
    "            dist = 0\n",
    "            \n",
    "        dist=int(abs(self.distance_to_obstacle//1))\n",
    "            \n",
    "\n",
    "        return dist,reward, done, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.19373106956481934\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.12430589]\n",
      " [ 1.6722469  -0.57470102]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  1\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 1.0\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.14995670318603516\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.12430589]\n",
      " [ 1.81708522 -0.58265507]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  2\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.9901493354116764\n",
      "LOOP START : time\n",
      "0.000997304916381836\n",
      "DISTANCE FOR REWARD\n",
      "0.23264145851135254\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.12430589]\n",
      " [ 1.81708522 -0.58981371]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  3\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.9803966865736877\n",
      "LOOP START : time\n",
      "0.0009975433349609375\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2624931335449219\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.12430589]\n",
      " [ 1.91340287 -0.59625649]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  4\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.970741078213023\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.23474860191345215\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.12430589]\n",
      " [ 1.91340287 -0.60205499]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  5\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.9611815447608\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.17152070999145508\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.12430589]\n",
      " [ 1.96148946 -0.60727364]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  6\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.9517171302557069\n",
      "LOOP START : time\n",
      "0.0009968280792236328\n",
      "DISTANCE FOR REWARD\n",
      "0.20667386054992676\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.12430589]\n",
      " [ 1.96148946 -0.61197043]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  7\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.9423468882484062\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.18627262115478516\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.17729945]\n",
      " [ 1.96148946 -0.61197043]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  8\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.9330698817068888\n",
      "LOOP START : time\n",
      "0.0009987354278564453\n",
      "DISTANCE FOR REWARD\n",
      "0.25832486152648926\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.22499366]\n",
      " [ 1.96148946 -0.61197043]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  9\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.9238851829227694\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.1761033535003662\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.22499366]\n",
      " [ 2.10546093 -0.61619754]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  10\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.9147918734185159\n",
      "LOOP START : time\n",
      "0.0009980201721191406\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.24280595779418945\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.22499366]\n",
      " [ 2.20120211 -0.62000194]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  11\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.9057890438555999\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19636249542236328\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.22499366]\n",
      " [ 2.24900091 -0.62342589]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  12\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.896875793943563\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.30143308639526367\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.22499366]\n",
      " [ 2.24900091 -0.62650746]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  13\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.888051232349986\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.16531968116760254\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.22499366]\n",
      " [ 2.34445516 -0.62928086]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  14\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.8793144766113556\n",
      "LOOP START : time\n",
      "0.0009953975677490234\n",
      "DISTANCE FOR REWARD\n",
      "0.20582890510559082\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.26791844]\n",
      " [ 2.34445516 -0.62928086]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  15\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.8706646530448178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.1871635913848877\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.26791844]\n",
      " [ 2.3921107  -0.63177693]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  16\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.8621008966608072\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.1560194492340088\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.26791844]\n",
      " [ 2.43971859 -0.63402338]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  17\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.8536223510765493\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.20511245727539062\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.26791844]\n",
      " [ 2.43971859 -0.6360452 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  18\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.8452281684304199\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.18480944633483887\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.30655075]\n",
      " [ 2.43971859 -0.6360452 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  19\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.8369175092971592\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.1667933464050293\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.30655075]\n",
      " [ 2.43971859 -0.63786483]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  20\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.8286895426039287\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.16492724418640137\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.30655075]\n",
      " [ 2.43971859 -0.6395025 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  21\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.820543445547202\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.224745512008667\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.05       -0.30655075]\n",
      " [ 2.43971859 -0.6409764 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  22\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.8124784035104852\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2596855163574219\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.1997002  -0.34131982]\n",
      " [ 2.43971859 -0.6409764 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  23\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.8044936099828537\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.23021864891052246\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.1997002  -0.34131982]\n",
      " [ 2.48727887 -0.64230291]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  24\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.7965882664783007\n",
      "LOOP START : time\n",
      "0.000997781753540039\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.16854357719421387\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.1997002  -0.34131982]\n",
      " [ 2.72436782 -0.64349677]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  25\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.7887615824558879\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2592461109161377\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.1997002  -0.34131982]\n",
      " [ 3.00731343 -0.64457124]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  26\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  7\n",
      "exploration rate: 0.7810127752406908\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19310665130615234\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.1997002  -0.34131982]\n",
      " [ 3.05430611 -0.64553827]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  27\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.7733410699455306\n",
      "LOOP START : time\n",
      "0.000997781753540039\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.177199125289917\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.1997002  -0.34131982]\n",
      " [ 3.14815056 -0.64640859]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  28\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.7657456993934846\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.23702692985534668\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.1997002  -0.34131982]\n",
      " [ 3.19500241 -0.64719189]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  29\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.7582259040411682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.18323945999145508\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.44820419 -0.37261199]\n",
      " [ 3.19500241 -0.64719189]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  30\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.7507809319027796\n",
      "LOOP START : time\n",
      "0.0009975433349609375\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21727585792541504\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.44820419 -0.37261199]\n",
      " [ 3.61456633 -0.64789685]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  31\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  10\n",
      "exploration rate: 0.7434100384749007\n",
      "LOOP START : time\n",
      "0.0009968280792236328\n",
      "DISTANCE FOR REWARD\n",
      "0.14394569396972656\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.44820419 -0.37261199]\n",
      " [ 3.61456633 -0.64853131]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  32\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.7361124866620463\n",
      "LOOP START : time\n",
      "0.00099945068359375\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.20721101760864258\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.44820419 -0.37261199]\n",
      " [ 3.84603011 -0.64910233]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  33\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.7288875467029541\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.20110034942626953\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.44820419 -0.40077494]\n",
      " [ 3.84603011 -0.64910233]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  34\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.7217344960976069\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.1814708709716797\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.44820419 -0.40077494]\n",
      " [ 4.03036925 -0.64961625]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  35\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 1]\n",
      "count_step =  5\n",
      "exploration rate: 0.7146526195349836\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.22405743598937988\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.44820419 -0.4261216 ]\n",
      " [ 4.03036925 -0.64961625]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  36\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.7076412088215263\n",
      "LOOP START : time\n",
      "0.0009965896606445312\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.16878437995910645\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.44820419 -0.4261216 ]\n",
      " [ 4.07633888 -0.65007878]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  37\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.7006995628103208\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.24173355102539062\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.44820419 -0.44893359]\n",
      " [ 4.07633888 -0.65007878]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  38\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.6938269873309811\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.1848771572113037\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.44820419 -0.44893359]\n",
      " [ 4.35119291 -0.65049505]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  39\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  7\n",
      "exploration rate: 0.6870227951202322\n",
      "LOOP START : time\n",
      "0.0009968280792236328\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.24285101890563965\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.44820419 -0.44893359]\n",
      " [ 4.53351443 -0.6508697 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  40\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 1]\n",
      "count_step =  5\n",
      "exploration rate: 0.680286305753183\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2525613307952881\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.44820419 -0.44893359]\n",
      " [ 4.62440193 -0.65120688]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  41\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.6736168455752829\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.17285585403442383\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.44820419 -0.44893359]\n",
      " [ 4.62440193 -0.65151034]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  42\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.6670137476349562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.22362279891967773\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.44820419 -0.44893359]\n",
      " [ 4.80563225 -0.65178346]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  43\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 1]\n",
      "count_step =  5\n",
      "exploration rate: 0.6604763516169062\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2080061435699463\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.44820419 -0.44893359]\n",
      " [ 4.89597579 -0.65202926]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  44\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.6540040037760834\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.1872100830078125\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.44820419 -0.44893359]\n",
      " [ 4.89597579 -0.65225049]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  45\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.64759605687231\n",
      "LOOP START : time\n",
      "0.0009963512420654297\n",
      "DISTANCE FOR REWARD\n",
      "0.17471075057983398\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.44820419 -0.44893359]\n",
      " [ 4.89597579 -0.65244959]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  46\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.6412518701055556\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2242889404296875\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.54725823 -0.46946438]\n",
      " [ 4.89597579 -0.65244959]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  47\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.6349708090518567\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.19954848289489746\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.54725823 -0.46946438]\n",
      " [ 4.89597579 -0.65262878]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  48\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.6287522455998737\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2024822235107422\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.54725823 -0.46946438]\n",
      " [ 4.98613874 -0.65279006]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  49\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.6225955578880794\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.20404863357543945\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.54725823 -0.46946438]\n",
      " [ 5.12104532 -0.6529352 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  50\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.616500130242572\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21672272682189941\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.54725823 -0.46946438]\n",
      " [ 5.43425712 -0.65306583]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  51\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  8\n",
      "exploration rate: 0.6104653531155071\n",
      "LOOP START : time\n",
      "0.0009970664978027344\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.260272741317749\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.54725823 -0.46946438]\n",
      " [ 5.56782069 -0.6531834 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  52\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.6044906230241432\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.15409588813781738\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.54725823 -0.46946438]\n",
      " [ 5.61225287 -0.65328921]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  53\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.5985753424904925\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2137761116027832\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.54725823 -0.46946438]\n",
      " [ 6.2734285  -0.65338444]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  54\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  16\n",
      "exploration rate: 0.5927189199815717\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.2410597801208496\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.54725823 -0.4879421 ]\n",
      " [ 6.2734285  -0.65338444]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  55\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.5869207698502498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.000997781753540039\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.20596742630004883\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.54725823 -0.4879421 ]\n",
      " [ 6.36083792 -0.65347015]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  56\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.5811803122766818\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21641254425048828\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.54725823 -0.4879421 ]\n",
      " [ 6.57859777 -0.65354728]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  57\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.5754969732103268\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.1867842674255371\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.54725823 -0.4879421 ]\n",
      " [ 6.66539716 -0.65361671]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  58\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.5698701843125418\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.15493488311767578\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.54725823 -0.4879421 ]\n",
      " [ 6.92475562 -0.65367919]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  59\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  7\n",
      "exploration rate: 0.564299382899748\n",
      "LOOP START : time\n",
      "0.000997781753540039\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21003317832946777\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.54725823 -0.4879421 ]\n",
      " [ 6.96783087 -0.65373542]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  60\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.558784011887162\n",
      "LOOP START : time\n",
      "0.0009975433349609375\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21325898170471191\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.54725823 -0.4879421 ]\n",
      " [ 7.13970152 -0.65378603]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  61\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 1]\n",
      "count_step =  5\n",
      "exploration rate: 0.5533235197330862\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.19471216201782227\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.54725823 -0.4879421 ]\n",
      " [ 7.13970152 -0.65383158]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  62\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.5479173603837548\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.2369706630706787\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.54725823 -0.4879421 ]\n",
      " [ 7.13970152 -0.65387257]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  63\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.5425649932187278\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21657848358154297\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.64611426 -0.50457204]\n",
      " [ 7.13970152 -0.65387257]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  64\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.5372658829968282\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.22705721855163574\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.64611426 -0.50457204]\n",
      " [ 7.13970152 -0.65390946]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  65\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.5320194998026181\n",
      "LOOP START : time\n",
      "0.0009980201721191406\n",
      "DISTANCE FOR REWARD\n",
      "0.19352340698242188\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.64611426 -0.51953899]\n",
      " [ 7.13970152 -0.65390946]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  66\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.5268253189934059\n",
      "LOOP START : time\n",
      "0.0009961128234863281\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19833827018737793\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.64611426 -0.51953899]\n",
      " [ 7.35357484 -0.65394267]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  67\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.5216828211467822\n",
      "LOOP START : time\n",
      "0.000997781753540039\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2024991512298584\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.64611426 -0.51953899]\n",
      " [ 7.43882504 -0.65397255]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  68\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.516591492008677\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19592857360839844\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.64611426 -0.51953899]\n",
      " [ 7.52390483 -0.65399945]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  69\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.5115508224419336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2618074417114258\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.64611426 -0.51953899]\n",
      " [ 7.56638093 -0.65402365]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  70\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.5065603083753949\n",
      "LOOP START : time\n",
      "0.0009982585906982422\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.18036150932312012\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.64611426 -0.51953899]\n",
      " [ 7.69355453 -0.65404544]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  71\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.5016194507534953\n",
      "LOOP START : time\n",
      "0.000995635986328125\n",
      "DISTANCE FOR REWARD\n",
      "0.2260723114013672\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.64611426 -0.51953899]\n",
      " [ 7.69355453 -0.65406505]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  72\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.49672775548635545\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.18695640563964844\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.64611426 -0.51953899]\n",
      " [ 7.82034699 -0.65408269]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  73\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.491884733400372\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.18956589698791504\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.64611426 -0.51953899]\n",
      " [ 7.90466411 -0.65409858]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  74\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.48708990018930043\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2375786304473877\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.64611426 -0.51953899]\n",
      " [ 7.98881269 -0.65411287]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  75\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.48234277636582407\n",
      "LOOP START : time\n",
      "0.0009965896606445312\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2042841911315918\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.64611426 -0.51953899]\n",
      " [ 8.15660554 -0.65412573]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  76\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 1]\n",
      "count_step =  5\n",
      "exploration rate: 0.47764288721360454\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21962761878967285\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.64611426 -0.51953899]\n",
      " [ 8.57316154 -0.65413731]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  77\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  11\n",
      "exploration rate: 0.47298976273981014\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.24581384658813477\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.64611426 -0.51953899]\n",
      " [ 8.65597379 -0.65414773]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  78\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.4683829376281158\n",
      "LOOP START : time\n",
      "0.0009968280792236328\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2594914436340332\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [ 8.65597379 -0.65414773]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  79\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.4638219511921713\n",
      "LOOP START : time\n",
      "0.0009992122650146484\n",
      "DISTANCE FOR REWARD\n",
      "0.19480514526367188\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [ 8.65597379 -0.65415711]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  80\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.4593063473295323\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.2430250644683838\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [ 8.65597379 -0.65416555]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  81\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.45483567447604933\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.24232172966003418\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [ 8.90341862 -0.65417315]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  82\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  7\n",
      "exploration rate: 0.4504094855607117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.23421335220336914\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [ 8.9445152  -0.65417998]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  83\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.44602733796093924\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21152806282043457\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [ 9.02658511 -0.65418613]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  84\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.4416887934583202\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2003798484802246\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [ 9.19023309 -0.65419167]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  85\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 1]\n",
      "count_step =  5\n",
      "exploration rate: 0.43739341819478894\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.13823890686035156\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [ 9.19023309 -0.65419666]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  86\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.43314078262923944\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2255721092224121\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [ 9.43448036 -0.65420114]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  87\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  7\n",
      "exploration rate: 0.4289304614945713\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.23586654663085938\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [ 9.51557084 -0.65420518]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  88\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.42476203375516264\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.20378470420837402\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [ 9.51557084 -0.65420881]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  89\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.42063508256476556\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.20133042335510254\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [ 9.67726581 -0.65421208]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  90\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 1]\n",
      "count_step =  5\n",
      "exploration rate: 0.41654919522482203\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.18754243850708008\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [ 9.71758854 -0.65421502]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  91\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.4125039631431931\n",
      "LOOP START : time\n",
      "0.0009975433349609375\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.17612123489379883\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [ 9.79811309 -0.65421767]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  92\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.40849898179329963\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.25200819969177246\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [ 9.79811309 -0.65422006]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  93\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.404533850673669\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2467954158782959\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [10.11860478 -0.6542222 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  94\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  9\n",
      "exploration rate: 0.40060817326788506\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.2432105541229248\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [10.11860478 -0.65422413]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  95\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.3967215570049359\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.19887971878051758\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [10.11860478 -0.65422587]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  96\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.3928736132199562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.24865269660949707\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [10.35729572 -0.65422743]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  97\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  7\n",
      "exploration rate: 0.38906395711536096\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.20196294784545898\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [10.63396354 -0.65422884]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  98\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  8\n",
      "exploration rate: 0.38529220772236483\n",
      "LOOP START : time\n",
      "0.0009970664978027344\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.24111580848693848\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [10.86957006 -0.65423011]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  99\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  7\n",
      "exploration rate: 0.3815579878628856\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.23152709007263184\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [10.94779179 -0.65423125]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  100\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.37786092411182526\n",
      "LOOP START : time\n",
      "0.0009975433349609375\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.26962709426879883\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [11.02585715 -0.65423228]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  101\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.3742006467597279\n",
      "LOOP START : time\n",
      "0.0009982585906982422\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.17269444465637207\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [11.25911818 -0.6542332 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  102\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  7\n",
      "exploration rate: 0.3705767897758081\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.22324657440185547\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [11.37522464 -0.65423403]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  103\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.3669889907713475\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.24017643928527832\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [11.41384941 -0.65423478]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  104\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.36343689096345594\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.262570858001709\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [11.49098313 -0.65423545]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  105\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.35992013513919235\n",
      "LOOP START : time\n",
      "0.000997304916381836\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19588041305541992\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [12.21610657 -0.65423606]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  106\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  20\n",
      "exploration rate: 0.35643837162004377\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19392013549804688\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [12.25389047 -0.6542366 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  107\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.3529912522267568\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTANCE FOR REWARD\n",
      "0.21671080589294434\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [12.89046643 -0.65423709]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  108\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  18\n",
      "exploration rate: 0.34957843224451957\n",
      "LOOP START : time\n",
      "0.0009980201721191406\n",
      "DISTANCE FOR REWARD\n",
      "0.24073028564453125\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [12.89046643 -0.65423754]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  109\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.34619957038848975\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2223646640777588\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.89239065 -0.53300924]\n",
      " [13.07564338 -0.65423793]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  110\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.34285432876966604\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2457263469696045\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.94149826 -0.54513247]\n",
      " [13.07564338 -0.65423793]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  111\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.3395423728610988\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2115492820739746\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.94149826 -0.54513247]\n",
      " [13.25989629 -0.65423829]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  112\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.3362633714644372\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.19660520553588867\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 0.94149826 -0.55604337]\n",
      " [13.25989629 -0.65423829]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  113\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.33301699667680906\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.23974370956420898\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.13743811 -0.56586318]\n",
      " [13.25989629 -0.65423829]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  114\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 1]\n",
      "count_step =  5\n",
      "exploration rate: 0.3298029238580304\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.18098831176757812\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.13743811 -0.56586318]\n",
      " [13.25989629 -0.65423861]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  115\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.3266208315981408\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.23134708404541016\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.13743811 -0.56586318]\n",
      " [13.44322977 -0.6542389 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  116\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.32347040168526264\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2311251163482666\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.13743811 -0.56586318]\n",
      " [13.47978654 -0.65423916]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  117\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.3203513190737793\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2508540153503418\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.13743811 -0.56586318]\n",
      " [13.55279045 -0.6542394 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  118\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.3172632718528302\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.1958601474761963\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.13743811 -0.56586318]\n",
      " [13.69836075 -0.65423961]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  119\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 1]\n",
      "count_step =  5\n",
      "exploration rate: 0.31420595121511996\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.22390532493591309\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.13743811 -0.56586318]\n",
      " [14.52417811 -0.6542398 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  120\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  24\n",
      "exploration rate: 0.311179051426037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.0009961128234863281\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.25299835205078125\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.13743811 -0.56586318]\n",
      " [14.70120281 -0.65423997]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  121\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.3081822697930801\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.20678257942199707\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.13743811 -0.56586318]\n",
      " [14.94755435 -0.65424012]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  122\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  8\n",
      "exploration rate: 0.3052153066355885\n",
      "LOOP START : time\n",
      "0.0009970664978027344\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.1782541275024414\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.13743811 -0.56586318]\n",
      " [15.15734394 -0.65424026]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  123\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  7\n",
      "exploration rate: 0.30227786525477407\n",
      "LOOP START : time\n",
      "0.0009980201721191406\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2297348976135254\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.13743811 -0.56586318]\n",
      " [15.71066477 -0.65424039]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  124\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  17\n",
      "exploration rate: 0.29936965190405085\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21927332878112793\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.13743811 -0.56586318]\n",
      " [16.11988122 -0.6542405 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  125\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  13\n",
      "exploration rate: 0.29649037575966014\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.22553348541259766\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.62387077 -0.57470102]\n",
      " [16.11988122 -0.6542405 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  126\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  11\n",
      "exploration rate: 0.29363974889158817\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2378835678100586\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.62387077 -0.57470102]\n",
      " [16.22141997 -0.6542406 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  127\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.2908174862347727\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.25473809242248535\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.62387077 -0.57470102]\n",
      " [16.85746945 -0.65424069]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  128\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  20\n",
      "exploration rate: 0.28802330556059597\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2541060447692871\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.76885407 -0.58265507]\n",
      " [16.85746945 -0.65424069]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  129\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.2852569274486622\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2721710205078125\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.76885407 -0.58265507]\n",
      " [17.08877233 -0.65424077]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  130\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  8\n",
      "exploration rate: 0.2825180752588548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.23405146598815918\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.76885407 -0.58265507]\n",
      " [18.02995546 -0.65424085]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  131\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  30\n",
      "exploration rate: 0.27980647510367246\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.1857898235321045\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.76885407 -0.58265507]\n",
      " [18.0619255  -0.65424091]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  132\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.2771218558208399\n",
      "LOOP START : time\n",
      "0.000997304916381836\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.23512959480285645\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.76885407 -0.58265507]\n",
      " [18.15764394 -0.65424097]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  133\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.27446394894619186\n",
      "LOOP START : time\n",
      "0.000997304916381836\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19759035110473633\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.76885407 -0.58265507]\n",
      " [18.81968869 -0.65424103]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  134\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  22\n",
      "exploration rate: 0.27183248868682575\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2647690773010254\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.76885407 -0.58265507]\n",
      " [18.97527875 -0.65424108]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  135\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.26922721189452276\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.22712469100952148\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.81708522 -0.58981371]\n",
      " [18.97527875 -0.65424108]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  136\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.2666478580394326\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.15561366081237793\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.81708522 -0.58981371]\n",
      " [18.97527875 -0.65424112]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  137\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.26409416918402034\n",
      "LOOP START : time\n",
      "0.0009999275207519531\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2280588150024414\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.81708522 -0.58981371]\n",
      " [19.06825987 -0.65424116]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  138\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.26156588995727226\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2038257122039795\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.81708522 -0.58981371]\n",
      " [19.28413357 -0.65424119]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  139\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  8\n",
      "exploration rate: 0.2590627675291589\n",
      "LOOP START : time\n",
      "0.000997781753540039\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.15880942344665527\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.81708522 -0.58981371]\n",
      " [19.37618905 -0.65424123]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  140\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.2565845515853515\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTANCE FOR REWARD\n",
      "0.18481922149658203\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.81708522 -0.58981371]\n",
      " [19.86251226 -0.65424125]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  141\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  17\n",
      "exploration rate: 0.2541309943021904\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19624567031860352\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.81708522 -0.58981371]\n",
      " [19.89264975 -0.65424128]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  142\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.25170185032190273\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.2044827938079834\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.81708522 -0.58981371]\n",
      " [19.89264975 -0.6542413 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  143\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.24929687672806608\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.19729185104370117\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.81708522 -0.58981371]\n",
      " [19.89264975 -0.65424132]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  144\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.246915833021317\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19245529174804688\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.81708522 -0.58981371]\n",
      " [20.34111239 -0.65424134]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  145\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  16\n",
      "exploration rate: 0.24455848109530054\n",
      "LOOP START : time\n",
      "0.000995635986328125\n",
      "DISTANCE FOR REWARD\n",
      "0.2480316162109375\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.81708522 -0.58981371]\n",
      " [20.34111239 -0.65424136]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  146\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.2422245852128597\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.25283241271972656\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.81708522 -0.58981371]\n",
      " [20.48911054 -0.65424137]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  147\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.23991391198246126\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.20193910598754883\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.81708522 -0.58981371]\n",
      " [20.5481028  -0.65424139]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  148\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.2376262303348566\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.23114752769470215\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.81708522 -0.58981371]\n",
      " [20.84129997 -0.6542414 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  149\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  11\n",
      "exploration rate: 0.2353613114999746\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.22809672355651855\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.81708522 -0.58981371]\n",
      " [20.92868862 -0.65424141]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  150\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.23311892898404435\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.20339751243591309\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.81708522 -0.58981371]\n",
      " [21.102681   -0.65424142]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  151\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  7\n",
      "exploration rate: 0.23089885854694553\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.16028213500976562\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 1.81708522 -0.58981371]\n",
      " [21.18928629 -0.65424143]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  152\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.22870087817978443\n",
      "LOOP START : time\n",
      "0.0009975433349609375\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTANCE FOR REWARD\n",
      "0.1916813850402832\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 2.77164346 -0.59625649]\n",
      " [21.18928629 -0.65424143]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  153\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  21\n",
      "exploration rate: 0.22652476808269262\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2040541172027588\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 3.2418074  -0.60205499]\n",
      " [21.18928629 -0.65424143]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  154\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  11\n",
      "exploration rate: 0.22437031064284702\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.23709416389465332\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 3.2418074  -0.60205499]\n",
      " [21.64681651 -0.65424144]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  155\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  17\n",
      "exploration rate: 0.22223729041270818\n",
      "LOOP START : time\n",
      "0.0009965896606445312\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.16787004470825195\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 3.2418074  -0.60205499]\n",
      " [21.78829918 -0.65424145]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  156\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.22012549408847562\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.25949597358703613\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 3.2418074  -0.60205499]\n",
      " [22.59508697 -0.65424145]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  157\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  30\n",
      "exploration rate: 0.21803471048875708\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.1712813377380371\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 3.2418074  -0.60205499]\n",
      " [22.89503826 -0.65424146]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  158\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  12\n",
      "exploration rate: 0.21596473053345028\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21406054496765137\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 3.2418074  -0.60205499]\n",
      " [23.03029229 -0.65424146]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  159\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.21391534722283462\n",
      "LOOP START : time\n",
      "0.000997304916381836\n",
      "DISTANCE FOR REWARD\n",
      "0.2496356964111328\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 3.2418074  -0.60205499]\n",
      " [23.03029229 -0.65424147]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  160\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.2118863556168713\n",
      "LOOP START : time\n",
      "0.000997781753540039\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.24743103981018066\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 3.2418074  -0.60205499]\n",
      " [23.40542374 -0.65424147]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  161\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  15\n",
      "exploration rate: 0.20987755281470882\n",
      "LOOP START : time\n",
      "0.000997781753540039\n",
      "[UPDATED]\n",
      "6 0.5 False None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTANCE FOR REWARD\n",
      "0.2023177146911621\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 3.2418074  -0.60205499]\n",
      " [23.43201832 -0.65424148]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  162\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.20788873793439305\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19805073738098145\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 3.2418074  -0.60205499]\n",
      " [23.67017593 -0.65424148]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  163\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  10\n",
      "exploration rate: 0.2059197120927785\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2543184757232666\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 3.2418074  -0.60205499]\n",
      " [23.69650576 -0.65424148]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  164\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.20397027838564025\n",
      "LOOP START : time\n",
      "0.000995635986328125\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2588067054748535\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 3.2418074  -0.60205499]\n",
      " [23.8539327  -0.65424149]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  165\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  7\n",
      "exploration rate: 0.20204024186798297\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.15746617317199707\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 3.2418074  -0.60205499]\n",
      " [24.34626223 -0.65424149]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  166\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  20\n",
      "exploration rate: 0.20012940953454655\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2220628261566162\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 3.2418074  -0.60205499]\n",
      " [24.52530057 -0.65424149]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  167\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  8\n",
      "exploration rate: 0.1982375903005053\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.15988612174987793\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 4.07633888 -0.60727364]\n",
      " [24.52530057 -0.65424149]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  168\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  19\n",
      "exploration rate: 0.19636459498235934\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.2400221824645996\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 4.07633888 -0.60727364]\n",
      " [24.52530057 -0.65424149]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  169\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.19451023627901587\n",
      "LOOP START : time\n",
      "0.0009982585906982422\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2221231460571289\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 4.07633888 -0.60727364]\n",
      " [24.65241957 -0.65424149]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  170\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.19267432875305937\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19155430793762207\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 4.57898091 -0.61197043]\n",
      " [24.65241957 -0.65424149]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  171\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  12\n",
      "exploration rate: 0.19085668881220733\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTANCE FOR REWARD\n",
      "0.22963547706604004\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 4.57898091 -0.61197043]\n",
      " [25.10481844 -0.6542415 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  172\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  19\n",
      "exploration rate: 0.1890571346909509\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19280219078063965\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 4.57898091 -0.61197043]\n",
      " [25.17942933 -0.6542415 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  173\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.1872754864323783\n",
      "LOOP START : time\n",
      "0.0009965896606445312\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.18912434577941895\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 4.57898091 -0.61197043]\n",
      " [25.64679981 -0.6542415 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  174\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  20\n",
      "exploration rate: 0.18551156587017906\n",
      "LOOP START : time\n",
      "0.0009984970092773438\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.23140311241149902\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 4.57898091 -0.61197043]\n",
      " [25.81676164 -0.6542415 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  175\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  8\n",
      "exploration rate: 0.1837651966108269\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.16892433166503906\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 4.57898091 -0.61197043]\n",
      " [26.48482979 -0.6542415 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  176\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  29\n",
      "exploration rate: 0.1820362040159407\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.3387625217437744\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 4.57898091 -0.61197043]\n",
      " [26.48482979 -0.6542415 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  177\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.18032441518482004\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.12883567810058594\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 4.57898091 -0.61197043]\n",
      " [26.88140555 -0.6542415 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  178\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  18\n",
      "exploration rate: 0.17862965893715535\n",
      "LOOP START : time\n",
      "0.0009970664978027344\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.17786121368408203\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 6.22965816 -0.61619754]\n",
      " [26.88140555 -0.6542415 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  179\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  38\n",
      "exploration rate: 0.17695176579590954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.0009975433349609375\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.24025797843933105\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 6.22965816 -0.61619754]\n",
      " [27.06570828 -0.6542415 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  180\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  9\n",
      "exploration rate: 0.1752905679703703\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2599673271179199\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 6.22965816 -0.61619754]\n",
      " [27.49756019 -0.6542415 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  181\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  20\n",
      "exploration rate: 0.17364589933937066\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2147657871246338\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 6.31715507 -0.62000194]\n",
      " [27.49756019 -0.6542415 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  182\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.172017595434677\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.15834355354309082\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 6.31715507 -0.62000194]\n",
      " [27.65460551 -0.6542415 ]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  183\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  8\n",
      "exploration rate: 0.17040549342454195\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.2001945972442627\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 6.31715507 -0.62000194]\n",
      " [27.65460551 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  184\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.16880943209742102\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.220489501953125\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 6.57859777 -0.62342589]\n",
      " [27.65460551 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  185\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  7\n",
      "exploration rate: 0.16722925184585147\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.23845267295837402\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 6.57859777 -0.62342589]\n",
      " [28.18477264 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  186\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  25\n",
      "exploration rate: 0.16566479465049133\n",
      "LOOP START : time\n",
      "0.0009984970092773438\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2675645351409912\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 6.57859777 -0.62342589]\n",
      " [28.27190274 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  187\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 1]\n",
      "count_step =  5\n",
      "exploration rate: 0.16411590406431734\n",
      "LOOP START : time\n",
      "0.0009975433349609375\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.24616551399230957\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 8.19844893 -0.62650746]\n",
      " [28.27190274 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  188\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  39\n",
      "exploration rate: 0.1625824251969801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.00099945068359375\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2550313472747803\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 8.19844893 -0.62650746]\n",
      " [28.6383401  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  189\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  18\n",
      "exploration rate: 0.16106420469931504\n",
      "LOOP START : time\n",
      "0.0009965896606445312\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.24202322959899902\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 8.19844893 -0.62650746]\n",
      " [28.74493499 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  190\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.15956109074800714\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2410144805908203\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 8.19844893 -0.62650746]\n",
      " [29.10339481 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  191\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  18\n",
      "exploration rate: 0.1580729330304087\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19167757034301758\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 8.19844893 -0.62650746]\n",
      " [29.14516713 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  192\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.15659958272950783\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.15601158142089844\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 8.19844893 -0.62650746]\n",
      " [29.7009882  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  193\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  28\n",
      "exploration rate: 0.15514089250904667\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21975088119506836\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 8.19844893 -0.62650746]\n",
      " [29.74156593 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  194\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.1536967164987875\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.1853184700012207\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 8.28201023 -0.62928086]\n",
      " [27.93654254 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  195\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  21\n",
      "exploration rate: 0.1522669102799259\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.24936938285827637\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 8.28201023 -0.62928086]\n",
      " [28.04663942 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  196\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.15085133087064845\n",
      "LOOP START : time\n",
      "0.0009949207305908203\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.18800735473632812\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 8.28201023 -0.62928086]\n",
      " [28.37364468 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  197\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  16\n",
      "exploration rate: 0.14944983671183457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19735312461853027\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [ 8.28201023 -0.62928086]\n",
      " [28.6103487  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  198\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  12\n",
      "exploration rate: 0.14806228765290044\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.20053935050964355\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [10.15848617 -0.63177693]\n",
      " [28.6103487  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  199\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  47\n",
      "exploration rate: 0.1466885449377839\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.16962385177612305\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [10.15848617 -0.63177693]\n",
      " [28.6103487  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  200\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.14532847119106862\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19176244735717773\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [10.15848617 -0.63177693]\n",
      " [28.80208733 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  201\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  10\n",
      "exploration rate: 0.1439819304042466\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.25201988220214844\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [10.27789123 -0.63402338]\n",
      " [28.80208733 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  202\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.14264878792211688\n",
      "LOOP START : time\n",
      "0.0009980201721191406\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19526004791259766\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [10.27789123 -0.63402338]\n",
      " [28.8656175  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  203\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.1413289104293205\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19031667709350586\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [10.27789123 -0.63402338]\n",
      " [29.13872202 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  204\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  14\n",
      "exploration rate: 0.14002216593700811\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.18761968612670898\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [10.27789123 -0.63402338]\n",
      " [29.2843136  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  205\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  8\n",
      "exploration rate: 0.13872842376964165\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2453293800354004\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [10.27789123 -0.63402338]\n",
      " [29.47001075 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  206\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  10\n",
      "exploration rate: 0.1374475545519262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.24909639358520508\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [10.27789123 -0.63402338]\n",
      " [29.9370426  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  207\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  24\n",
      "exploration rate: 0.13617943019587256\n",
      "LOOP START : time\n",
      "0.0009980201721191406\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.35916996002197266\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [10.43654149 -0.6360452 ]\n",
      " [28.27981174 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  208\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  21\n",
      "exploration rate: 0.13492392388798838\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.20791339874267578\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [10.43654149 -0.6360452 ]\n",
      " [28.71011332 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  209\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  21\n",
      "exploration rate: 0.13368091007659658\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.20843124389648438\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [10.43654149 -0.6360452 ]\n",
      " [29.0691604  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  210\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  18\n",
      "exploration rate: 0.1324502644592803\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.18677902221679688\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [12.02661902 -0.63786483]\n",
      " [29.0691604  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  211\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  42\n",
      "exploration rate: 0.13123186397045208\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.18349218368530273\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [12.02661902 -0.63786483]\n",
      " [29.09009124 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  212\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.13002558676904785\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTANCE FOR REWARD\n",
      "0.18122029304504395\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [13.37000641 -0.6395025 ]\n",
      " [29.09009124 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  213\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  37\n",
      "exploration rate: 0.12883131222634217\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.18644976615905762\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [13.37000641 -0.6395025 ]\n",
      " [29.13189014 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  214\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.12764892091388555\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.14087343215942383\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [13.37000641 -0.6395025 ]\n",
      " [29.91038255 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  215\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  39\n",
      "exploration rate: 0.1264782945915614\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19002676010131836\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [13.37000641 -0.6395025 ]\n",
      " [29.93047217 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  216\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.125319316195762\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.20459651947021484\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [13.37000641 -0.6395025 ]\n",
      " [29.93047217 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  217\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.12417186982768189\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.234405517578125\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [14.59509427 -0.6409764 ]\n",
      " [29.93047217 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  218\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  35\n",
      "exploration rate: 0.12303584074172813\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2741560935974121\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [14.59509427 -0.6409764 ]\n",
      " [30.05058869 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  219\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  7\n",
      "exploration rate: 0.12191111533404535\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.17239022254943848\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [14.59509427 -0.6409764 ]\n",
      " [30.15013646 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  220\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.12079758113115559\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTANCE FOR REWARD\n",
      "0.21500182151794434\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [15.43511154 -0.64230291]\n",
      " [30.15013646 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  221\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  25\n",
      "exploration rate: 0.11969512677871053\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2463827133178711\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [15.43511154 -0.64230291]\n",
      " [30.18981633 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  222\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.11860364203035628\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2690727710723877\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [15.43511154 -0.64230291]\n",
      " [30.42623541 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  223\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  13\n",
      "exploration rate: 0.11752301773670837\n",
      "LOOP START : time\n",
      "0.0009989738464355469\n",
      "DISTANCE FOR REWARD\n",
      "0.19699931144714355\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [15.43511154 -0.64349677]\n",
      " [30.42623541 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  224\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.116453145834437\n",
      "LOOP START : time\n",
      "0.001001596450805664\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.23429369926452637\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [15.43511154 -0.64349677]\n",
      " [30.46536337 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  225\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.11539391933546027\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21777725219726562\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [15.91588712 -0.64457124]\n",
      " [29.63583287 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  226\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  53\n",
      "exploration rate: 0.11434523231624569\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2213740348815918\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [15.91588712 -0.64457124]\n",
      " [29.97926798 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  227\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  18\n",
      "exploration rate: 0.11330697990721733\n",
      "LOOP START : time\n",
      "0.000997304916381836\n",
      "DISTANCE FOR REWARD\n",
      "0.18002986907958984\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [15.91588712 -0.64457124]\n",
      " [29.97926798 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  228\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.1122790582822692\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.17583942413330078\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [15.91588712 -0.64457124]\n",
      " [30.07917163 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  229\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.11126136464838206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.20084118843078613\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [15.91588712 -0.64457124]\n",
      " [30.39552551 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  230\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  17\n",
      "exploration rate: 0.11025379723534455\n",
      "LOOP START : time\n",
      "0.000997781753540039\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.1994621753692627\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [15.91588712 -0.64457124]\n",
      " [30.66821126 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  231\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  15\n",
      "exploration rate: 0.10925625528557566\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.1892998218536377\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [15.91588712 -0.64457124]\n",
      " [30.87980087 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  232\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  12\n",
      "exploration rate: 0.1082686390440492\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2118997573852539\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [15.91588712 -0.64457124]\n",
      " [30.93710413 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  233\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.10729084974831793\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19861888885498047\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [15.91588712 -0.64457124]\n",
      " [31.14575066 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  234\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  12\n",
      "exploration rate: 0.10632278961863742\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.20639562606811523\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.11988122 -0.64553827]\n",
      " [30.12561653 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  235\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  36\n",
      "exploration rate: 0.10536436184818809\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.16307663917541504\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.28894335 -0.64640859]\n",
      " [30.12561653 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  236\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.10441547059339411\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.17183709144592285\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.28894335 -0.64640859]\n",
      " [30.44123283 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  237\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  17\n",
      "exploration rate: 0.10347602096433932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.0009975433349609375\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21271681785583496\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.28894335 -0.64640859]\n",
      " [30.65530676 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  238\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  12\n",
      "exploration rate: 0.10254591901527788\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.22758269309997559\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.28894335 -0.64640859]\n",
      " [30.92437912 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  239\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  15\n",
      "exploration rate: 0.10162507173523985\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.26474642753601074\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.28894335 -0.64640859]\n",
      " [31.00056723 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  240\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 1]\n",
      "count_step =  5\n",
      "exploration rate: 0.10071338703872978\n",
      "LOOP START : time\n",
      "0.0009944438934326172\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.1971755027770996\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.62454096 -0.64719189]\n",
      " [30.16804593 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  241\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  48\n",
      "exploration rate: 0.09981077375651838\n",
      "LOOP START : time\n",
      "0.0009984970092773438\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.18070459365844727\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.95679765 -0.64789685]\n",
      " [29.46127332 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  242\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  47\n",
      "exploration rate: 0.09891714162652515\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.12200331687927246\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.95679765 -0.64789685]\n",
      " [29.94857543 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  243\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  25\n",
      "exploration rate: 0.09803240128479247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.17610931396484375\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.95679765 -0.64789685]\n",
      " [30.18787353 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  244\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  13\n",
      "exploration rate: 0.09715646425654882\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.18570852279663086\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.95679765 -0.64789685]\n",
      " [30.44389148 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  245\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  14\n",
      "exploration rate: 0.09628924294736148\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2630603313446045\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.95679765 -0.64789685]\n",
      " [30.6966011  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  246\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  14\n",
      "exploration rate: 0.09543065063437678\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.2269296646118164\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.95679765 -0.64789685]\n",
      " [30.6966011  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  247\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.09458060145764802\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21565723419189453\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.95679765 -0.64789685]\n",
      " [31.04112455 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  248\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  19\n",
      "exploration rate: 0.09373901041154904\n",
      "LOOP START : time\n",
      "0.000997781753540039\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2340860366821289\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.95679765 -0.64789685]\n",
      " [31.1545938  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  249\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  7\n",
      "exploration rate: 0.092905793336274\n",
      "LOOP START : time\n",
      "0.0009942054748535156\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.17658472061157227\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.95679765 -0.64789685]\n",
      " [31.58330346 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  250\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  24\n",
      "exploration rate: 0.09208086690942092\n",
      "LOOP START : time\n",
      "0.0009999275207519531\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.15836262702941895\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.95679765 -0.64789685]\n",
      " [32.23483243 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  251\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  37\n",
      "exploration rate: 0.0912641486376598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2676122188568115\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.95679765 -0.64789685]\n",
      " [32.41168681 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  252\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  11\n",
      "exploration rate: 0.09045555684848275\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.17331910133361816\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.95679765 -0.64789685]\n",
      " [32.58678057 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  253\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  11\n",
      "exploration rate: 0.08965501068203711\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.26653289794921875\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.95679765 -0.64789685]\n",
      " [33.20280945 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  254\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  37\n",
      "exploration rate: 0.08886243008303903\n",
      "LOOP START : time\n",
      "0.0009968280792236328\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.26903867721557617\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.95679765 -0.64789685]\n",
      " [33.2866276  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  255\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 1]\n",
      "count_step =  6\n",
      "exploration rate: 0.08807773579276819\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.1992032527923584\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [16.95679765 -0.64789685]\n",
      " [34.1656422  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  256\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  55\n",
      "exploration rate: 0.08730084934114159\n",
      "LOOP START : time\n",
      "0.0009975433349609375\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.20644140243530273\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [17.05582816 -0.64853131]\n",
      " [34.1656422  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  257\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 1]\n",
      "count_step =  4\n",
      "exploration rate: 0.08653169303886674\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.31484413146972656\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [17.05582816 -0.64853131]\n",
      " [34.27615074 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  258\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  8\n",
      "exploration rate: 0.0857701899696724\n",
      "LOOP START : time\n",
      "0.000997781753540039\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTANCE FOR REWARD\n",
      "0.1965329647064209\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [17.05582816 -0.64853131]\n",
      " [34.4793388  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  259\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  14\n",
      "exploration rate: 0.08501626398261702\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.20954346656799316\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [17.05582816 -0.64853131]\n",
      " [34.4793388  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  260\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.0842698396844737\n",
      "LOOP START : time\n",
      "0.0\n",
      "DISTANCE FOR REWARD\n",
      "0.22281193733215332\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [17.05582816 -0.64853131]\n",
      " [34.4793388  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  261\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [-1]\n",
      "action_current_episode = [1]\n",
      "count_step =  1\n",
      "exploration rate: 0.08353084243219053\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.1602940559387207\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [17.05582816 -0.64853131]\n",
      " [34.60307038 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  262\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  9\n",
      "exploration rate: 0.08279919832542652\n",
      "LOOP START : time\n",
      "0.0009996891021728516\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.22519421577453613\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [17.05582816 -0.64853131]\n",
      " [34.83241463 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  263\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  16\n",
      "exploration rate: 0.08207483419916123\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.20016217231750488\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [17.05582816 -0.64853131]\n",
      " [35.16262105 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  264\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  23\n",
      "exploration rate: 0.08135767761637844\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21480345726013184\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [17.05582816 -0.64853131]\n",
      " [35.36899954 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  265\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  15\n",
      "exploration rate: 0.08064765686082219\n",
      "LOOP START : time\n",
      "0.0009965896606445312\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.17699885368347168\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [17.05582816 -0.64853131]\n",
      " [35.60134799 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  266\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  17\n",
      "exploration rate: 0.0799447009298253\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2623262405395508\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [17.74078111 -0.64910233]\n",
      " [35.60134799 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  267\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  22\n",
      "exploration rate: 0.0792487395272088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.16878056526184082\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [17.74078111 -0.64910233]\n",
      " [35.61574664 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  268\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, -1]\n",
      "action_current_episode = [0, 1]\n",
      "count_step =  2\n",
      "exploration rate: 0.07855970305625253\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.24895286560058594\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [18.22129681 -0.64961625]\n",
      " [34.1676622  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  269\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  41\n",
      "exploration rate: 0.07787752261273513\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.23599624633789062\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [18.22129681 -0.64961625]\n",
      " [34.83434957 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  270\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  44\n",
      "exploration rate: 0.07720212997804382\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.17076969146728516\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [18.22129681 -0.64961625]\n",
      " [35.47296326 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  271\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  44\n",
      "exploration rate: 0.07653345761235225\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.26557040214538574\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [18.78847716 -0.65007878]\n",
      " [34.11529262 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  272\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  46\n",
      "exploration rate: 0.07587143864786668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21563100814819336\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [18.78847716 -0.65007878]\n",
      " [34.14704615 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  273\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.07521600688213892\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.16630172729492188\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [19.22260956 -0.65049505]\n",
      " [33.25686758 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  274\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  60\n",
      "exploration rate: 0.07456709677144624\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19016170501708984\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [19.22260956 -0.65049505]\n",
      " [33.80064124 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  275\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  34\n",
      "exploration rate: 0.07392464342423678\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2337801456451416\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [19.37618905 -0.6508697 ]\n",
      " [33.19556595 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  276\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  65\n",
      "exploration rate: 0.0732885825946405\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTANCE FOR REWARD\n",
      "0.23698639869689941\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [19.37618905 -0.6508697 ]\n",
      " [33.51199286 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  277\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  20\n",
      "exploration rate: 0.07265885067604429\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.009330987930297852\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [19.68105276 -0.65120688]\n",
      " [33.51199286 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  278\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  11\n",
      "exploration rate: 0.07203538469473161\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.27387380599975586\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [19.68105276 -0.65120688]\n",
      " [33.69245681 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  279\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  12\n",
      "exploration rate: 0.0714181223035847\n",
      "LOOP START : time\n",
      "0.0009987354278564453\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.1924574375152588\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [19.68105276 -0.65120688]\n",
      " [33.72505559 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  280\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 1]\n",
      "count_step =  3\n",
      "exploration rate: 0.07080700177585013\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21087980270385742\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [20.40040051 -0.65151034]\n",
      " [32.74135674 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  281\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  53\n",
      "exploration rate: 0.07020196199896576\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.20966219902038574\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [20.40040051 -0.65151034]\n",
      " [33.1676847  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  282\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  26\n",
      "exploration rate: 0.06960294246844981\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19483184814453125\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [20.84129997 -0.65178346]\n",
      " [33.1676847  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  283\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  16\n",
      "exploration rate: 0.06900988328184997\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTANCE FOR REWARD\n",
      "0.18711233139038086\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [21.33305204 -0.65202926]\n",
      " [32.42684982 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  284\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  50\n",
      "exploration rate: 0.06842272513275337\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.4183063507080078\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [21.33305204 -0.65202926]\n",
      " [33.31773529 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  285\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  53\n",
      "exploration rate: 0.06784140930485581\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21234989166259766\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [21.33305204 -0.65202926]\n",
      " [33.53330827 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  286\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  14\n",
      "exploration rate: 0.06726587766609007\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.1847221851348877\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [21.33305204 -0.65202926]\n",
      " [33.82720271 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  287\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  19\n",
      "exploration rate: 0.06669607266281269\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.17392277717590332\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [21.33305204 -0.65202926]\n",
      " [34.70841568 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  288\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  57\n",
      "exploration rate: 0.06613193731404843\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTANCE FOR REWARD\n",
      "0.2677791118621826\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [21.70349452 -0.65225049]\n",
      " [34.70841568 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  289\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  14\n",
      "exploration rate: 0.06557341520579238\n",
      "LOOP START : time\n",
      "0.000997304916381836\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.20311665534973145\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [21.70349452 -0.65225049]\n",
      " [34.99635568 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  290\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  20\n",
      "exploration rate: 0.06502045048536823\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21785187721252441\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [22.75910592 -0.65244959]\n",
      " [34.26397863 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  291\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  83\n",
      "exploration rate: 0.06447298785584316\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.21060538291931152\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [24.52530057 -0.65262878]\n",
      " [34.26397863 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  292\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  68\n",
      "exploration rate: 0.06393097257049796\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTANCE FOR REWARD\n",
      "0.2003183364868164\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [24.52530057 -0.65262878]\n",
      " [34.66804166 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  293\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  27\n",
      "exploration rate: 0.06339435042735246\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.20237994194030762\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [24.67776715 -0.65279006]\n",
      " [34.15407827 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  294\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  43\n",
      "exploration rate: 0.06286306776374512\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.19695281982421875\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [25.57359385 -0.6529352 ]\n",
      " [33.4301354  -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  295\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  52\n",
      "exploration rate: 0.06233707145096687\n",
      "LOOP START : time\n",
      "0.0009975433349609375\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2608816623687744\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [25.57359385 -0.6529352 ]\n",
      " [33.83944896 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  296\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  26\n",
      "exploration rate: 0.06181630888894806\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.1731860637664795\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [25.57359385 -0.6529352 ]\n",
      " [34.04828021 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  297\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  14\n",
      "exploration rate: 0.061300728000998575\n",
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "DISTANCE FOR REWARD\n",
      "0.2297346591949463\n",
      "[UPDATED]\n",
      "0 -1 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.62387077 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [25.57359385 -0.6529352 ]\n",
      " [34.42675093 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  298\n",
      "distance to obstacle =  0\n",
      "rewards_current_episode =  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -1]\n",
      "action_current_episode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "count_step =  25\n",
      "exploration rate: 0.06079027722859992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP START : time\n",
      "0.0\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "6 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "[UPDATED]\n",
      "5 0.5 False None\n",
      "exploration rate: 0.060284905526249516\n",
      "LOOP START : time\n",
      "0.0009963512420654297\n",
      "DISTANCE FOR REWARD\n",
      "1.5856802463531494\n",
      "[UPDATED]\n",
      "1 1.5 True None\n",
      "[[ 0.349251    0.20832748]\n",
      " [ 1.77224689 -0.73807624]\n",
      " [ 0.99055676 -0.59842658]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.06542415]\n",
      " [27.15730793 -0.6529352 ]\n",
      " [34.00748832 -0.65424151]\n",
      " [ 1.71618289  0.06933105]\n",
      " [49.99300531  1.96897879]\n",
      " [49.37267201  0.27837086]]\n",
      "destroying actors\n",
      "episode number =  299\n",
      "distance to obstacle =  1\n",
      "rewards_current_episode =  [1.5]\n",
      "action_current_episode = [0]\n",
      "count_step =  1\n",
      "exploration rate: 0.05978456235635595\n"
     ]
    }
   ],
   "source": [
    "env = CarEnv()\n",
    "\n",
    "# action มี 2 อันคือ เบรคและเร่งเต็มที่, state มี 13 อัน คือ ชน, 0-1, 1-2, 2-3, 3-4, 4-5, 5-6, 6-7, 7-8, 8-9, 9-10, 10-11, >11\n",
    "action_space_size = 2\n",
    "state_space_size = 10\n",
    "#q_table = np.zeros((state_space_size, action_space_size))\n",
    "\n",
    "num_episodes = 300\n",
    "max_steps_per_episode = 100\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.01\n",
    "\n",
    "count = 0\n",
    "rewards_all_episodes = []\n",
    "action_all_episodes = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "      \n",
    "    print('LOOP START : time')\n",
    "    print(time.time()-env.episode_start)\n",
    "    done = False\n",
    "    rewards_current_episode = []\n",
    "    action_current_episode = []\n",
    "    count_step=0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) \n",
    "        else:\n",
    "            action = np.random.randint(0, high=2, size=None, dtype='int')\n",
    "        action_current_episode.append(action)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        print('[UPDATED]')\n",
    "        print(new_state,reward,done,info)\n",
    "        if new_state > 8 :\n",
    "            new_state = 8\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        count_step+=1\n",
    "        state = new_state\n",
    "        rewards_current_episode.append(reward)\n",
    "                    \n",
    "        if done == True:\n",
    "            print(q_table)\n",
    "            print('destroying actors')\n",
    "            for actor in env.actor_list:\n",
    "                actor.destroy()\n",
    "            count+=1\n",
    "            print('episode number = ',count)\n",
    "            print('distance to obstacle = ',state)\n",
    "            print('rewards_current_episode = ',rewards_current_episode)\n",
    "            print('action_current_episode =', action_current_episode)\n",
    "            print('count_step = ', count_step)\n",
    "            break            \n",
    "        \n",
    "    # Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "          (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    print('exploration rate: '+str(exploration_rate))\n",
    "\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    action_all_episodes.append(action_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxx()\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 : -0.55\n",
      "20 : -1.2\n",
      "30 : -1.0\n",
      "40 : -0.55\n",
      "50 : -0.8\n",
      "60 : 0.3\n",
      "70 : 0.1\n",
      "80 : 0.65\n",
      "90 : 0.9\n",
      "100 : 1.5\n",
      "110 : 3.3\n",
      "120 : 4.55\n",
      "130 : 7.95\n",
      "140 : 10.6\n",
      "150 : 12.2\n",
      "160 : 16.45\n",
      "170 : 19.45\n",
      "180 : 25.85\n",
      "190 : 30.95\n",
      "200 : 37.2\n",
      "210 : 42.4\n",
      "220 : 49.6\n",
      "230 : 55.1\n",
      "240 : 60.3\n",
      "250 : 69.4\n",
      "260 : 77.1\n",
      "270 : 85.1\n",
      "280 : 98.5\n",
      "290 : 113.1\n",
      "300 : 133.95\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a=rewards_all_episodes\n",
    "n = 0 \n",
    "count = 0\n",
    "n_list=[]\n",
    "count_list=[]\n",
    "for i in a :\n",
    "    for j in i:\n",
    "        count += j\n",
    "    count_list.append(sum(i))\n",
    "    n+=1\n",
    "    n_list.append(n)\n",
    "    if n%10 == 0:\n",
    "        print(n,':',str(count/10)) \n",
    "        \n",
    "ax=plt.scatter(n_list,count_list,marker='o')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 1, 3, 1, 2, 1, 1, 1, 4, 3, 2, 1, 3, 1, 2, 2, 1, 1, 1, 1, 1, 4, 2, 6, 7, 2, 3, 2, 6, 10, 1, 6, 1, 5, 1, 2, 1, 7, 5, 3, 1, 5, 3, 1, 1, 3, 1, 3, 4, 8, 4, 2, 16, 1, 3, 6, 3, 7, 2, 5, 1, 1, 3, 1, 1, 6, 3, 3, 2, 4, 1, 4, 3, 3, 5, 11, 3, 6, 1, 1, 7, 2, 3, 5, 1, 7, 3, 1, 5, 2, 3, 1, 9, 1, 1, 7, 8, 7, 3, 3, 7, 4, 2, 3, 20, 2, 18, 1, 6, 2, 6, 1, 5, 1, 6, 2, 3, 5, 24, 6, 8, 7, 17, 13, 11, 4, 20, 4, 8, 30, 2, 4, 22, 6, 2, 1, 4, 8, 4, 17, 2, 1, 1, 16, 1, 6, 3, 11, 4, 7, 4, 21, 11, 17, 6, 30, 12, 6, 1, 15, 2, 10, 2, 7, 20, 8, 19, 1, 6, 12, 19, 4, 20, 8, 29, 1, 18, 38, 9, 20, 3, 8, 1, 7, 25, 5, 39, 18, 6, 18, 3, 28, 3, 21, 6, 16, 12, 47, 1, 10, 4, 4, 14, 8, 10, 24, 21, 21, 18, 42, 2, 37, 3, 39, 2, 1, 35, 7, 6, 25, 3, 13, 1, 3, 53, 18, 1, 6, 17, 15, 12, 4, 12, 36, 6, 17, 12, 15, 5, 48, 47, 25, 13, 14, 14, 1, 19, 7, 24, 37, 11, 11, 37, 6, 55, 4, 8, 14, 1, 1, 9, 16, 23, 15, 17, 22, 2, 41, 44, 44, 46, 3, 60, 34, 65, 20, 11, 12, 3, 53, 26, 16, 50, 53, 14, 19, 57, 14, 20, 83, 68, 27, 43, 52, 26, 14, 25, 100, 1]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300]\n"
     ]
    }
   ],
   "source": [
    "b=action_all_episodes\n",
    "step = []\n",
    "episode=[]\n",
    "n=1\n",
    "for i in b:\n",
    "    step.append(len(i))\n",
    "    episode.append(n)\n",
    "    n+=1\n",
    "print(step)\n",
    "print(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax=plt.scatter(episode,step,marker='o')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Step_per_episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.349251  ,  0.20832748],\n",
       "       [ 1.77224689, -0.73807624],\n",
       "       [ 0.99055676, -0.59842658],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        , -0.06542415],\n",
       "       [27.15730793, -0.6529352 ],\n",
       "       [34.00748832, -0.65424151],\n",
       "       [ 1.71618289,  0.06933105],\n",
       "       [49.99300531,  1.96897879],\n",
       "       [49.37267201,  0.27837086]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
