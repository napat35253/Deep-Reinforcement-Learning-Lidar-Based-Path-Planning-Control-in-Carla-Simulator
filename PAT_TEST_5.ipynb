{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T   \n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import psutil\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "try:\n",
    "    sys.path.append(glob.glob('../carla/dist/carla-*%d.%d-%s.egg' % (\n",
    "        sys.version_info.major,\n",
    "        sys.version_info.minor,\n",
    "        'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])\n",
    "except IndexError:\n",
    "    pass\n",
    "import carla\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "#import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "SECONDS_PER_EPISODE = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xxx():\n",
    "    env.world.wait_for_tick()\n",
    "    for x in list(env.world.get_actors()):\n",
    "        if x.type_id == 'vehicle.tesla.model3' or x.type_id == 'sensor.lidar.ray_cast' or x.type_id == 'sensor.other.collision':\n",
    "            x.destroy()\n",
    "            \n",
    "def lidar_line(points,degree,width):\n",
    "    angle = degree*(2*np.pi)/360\n",
    "    points_l = points\n",
    "    points_l = points_l[np.logical_and(points_l[:,2] > -1.75, points_l[:,2] < 1000)] #z\n",
    "    points_l = points_l[np.logical_and(np.tan(angle)*points_l[:,0]+width*np.sqrt(1+np.tan(angle)**2)>=points_l[:,1], np.tan(angle)*points_l[:,0]-width*np.sqrt(1+np.tan(angle)**2)<=points_l[:,1])] #y\n",
    "    if 180>degree >0:\n",
    "        points_l = points_l[np.logical_and(points_l[:,1]>0, points_l[:,1]<1000)] #y>0\n",
    "    if 180<degree<360:\n",
    "        points_l = points_l[np.logical_and(points_l[:,1]<0, points_l[:,1] > -1000)] #x\n",
    "    if degree == 0 or degree == 360:\n",
    "        points_l = points_l[np.logical_and(points_l[:,0]>0,points_l[:,0] <1000 )] #x\n",
    "    if degree == 180:\n",
    "        points_l = points_l[np.logical_and(points_l[:,0] >-1000 , points_l[:,0]<0 )]\n",
    "    return  points_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValues():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "    @staticmethod        \n",
    "    def get_next(target_net, next_states):                \n",
    "        final_state_locations = next_states.flatten(start_dim=1) \\\n",
    "        .max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        non_final_states = next_states[non_final_state_locations]\n",
    "        batch_size = next_states.shape[0]\n",
    "        values = torch.zeros(batch_size).to(QValues.device)\n",
    "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self,state_size):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.fc1 = nn.Linear(in_features=state_size, out_features=24)   \n",
    "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=2)\n",
    "        \n",
    "        \n",
    "    def forward(self, t):\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        return t\n",
    "    \n",
    "class ReplayMemory():\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size\n",
    "    \n",
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "        \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * \\\n",
    "            math.exp(-1. * current_step * self.decay)\n",
    "    \n",
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions,device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "        \n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            print(action)\n",
    "            return torch.tensor([action]).to(self.device) # explore      \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                print(policy_net(state).argmax().to(self.device))\n",
    "                return policy_net(state).argmax().to(self.device) # exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "target_update = 10\n",
    "memory_size = 100000\n",
    "lr = 0.001\n",
    "num_episodes = 1000\n",
    "\n",
    "num_action=2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent = Agent(strategy, num_action , device)\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "state_size=3\n",
    "\n",
    "\n",
    "policy_net = DQN(state_size).to(device)\n",
    "target_net = DQN(state_size).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarEnv:\n",
    "    #BRAKE_AMT = 1.0\n",
    "    actor_list = []\n",
    "    collision_hist = []\n",
    "    pt_cloud = []\n",
    "    pt_cloud_filtered = []\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = carla.Client('localhost', 2000)\n",
    "        self.client.set_timeout(2.0)\n",
    "        self.world = self.client.get_world()\n",
    "        blueprint_library = self.world.get_blueprint_library()\n",
    "        self.model_3 = blueprint_library.filter('model3')[0]\n",
    "        self.truck_2 = blueprint_library.filter('carlamotors')[0]\n",
    "        #settings = self.world.get_settings()\n",
    "        #settings.no_rendering_mode = True\n",
    "        #self.world.apply_settings(settings)\n",
    "                     \n",
    "    def reset(self):\n",
    "        self.collision_hist = []\n",
    "        self.actor_list = []\n",
    "        self.pt_cloud = []\n",
    "        self.pt_cloud_filtered = []\n",
    "        place=random.uniform(110,150)\n",
    "        ##print('Location: ',str(place))\n",
    "        #transform = carla.Transform(carla.Location(-120,place,3),carla.Rotation(0,-90,0))\n",
    "        transform = carla.Transform(carla.Location(246,-36,3),carla.Rotation(0,-90,0))        \n",
    "        self.flag = 0\n",
    "        self.vehicle = self.world.spawn_actor(self.model_3, transform)\n",
    "        self.flag = 1\n",
    "        \n",
    "        self.actor_list.append(self.vehicle)\n",
    "     \n",
    "\n",
    "        self.lidar_sensor = self.world.get_blueprint_library().find('sensor.lidar.ray_cast')\n",
    "        self.lidar_sensor.set_attribute('points_per_second', '100000')\n",
    "        self.lidar_sensor.set_attribute('channels', '32')\n",
    "        self.lidar_sensor.set_attribute('range', '10000')\n",
    "        self.lidar_sensor.set_attribute('upper_fov', '10')\n",
    "        self.lidar_sensor.set_attribute('lower_fov', '-10')\n",
    "        self.lidar_sensor.set_attribute('rotation_frequency', '60')\n",
    "        \n",
    "        transform = carla.Transform(carla.Location(x=0, z=1.9))\n",
    "        time.sleep(0.01)\n",
    "\n",
    "        self.sensor = self.world.spawn_actor(self.lidar_sensor, transform, attach_to=self.vehicle)\n",
    "     \n",
    "        self.actor_list.append(self.sensor)\n",
    "        self.sensor.listen(lambda data: self.process_lidar(data))\n",
    "\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=1, brake=0.0))\n",
    "        self.episode_start = time.time()\n",
    "   \n",
    "        time.sleep(0.4) # sleep to get things started and to not detect a collision when the car spawns/falls from sky.\n",
    "        \n",
    "        transform2 = carla.Transform(carla.Location(x=2.5, z=0.7))\n",
    "        colsensor = self.world.get_blueprint_library().find('sensor.other.collision')\n",
    "    \n",
    "        #time.sleep(0.1)\n",
    "        self.colsensor = self.world.spawn_actor(colsensor, transform2, attach_to=self.vehicle)\n",
    "   \n",
    "        self.actor_list.append(self.colsensor)\n",
    "        self.colsensor.listen(lambda event: self.collision_data(event))\n",
    "\n",
    "        while self.distance_to_obstacle_f is None:\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        self.episode_start = time.time()\n",
    "        \n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=1, brake=0.0))\n",
    "\n",
    "        xx = self.distance_to_obstacle_f\n",
    "        yy = self.distance_to_obstacle_r\n",
    "        zz = self.distance_to_obstacle_l\n",
    "   \n",
    "        \n",
    "        state_=np.array([xx,yy,zz])\n",
    "        return state_\n",
    "\n",
    "    def collision_data(self, event):\n",
    "        self.collision_hist.append(event)\n",
    "\n",
    "    def process_lidar(self, raw):\n",
    "        points = np.frombuffer(raw.raw_data, dtype=np.dtype('f4'))\n",
    "        points = np.reshape(points, (int(points.shape[0] / 3), 3))*np.array([1,-1,-1])\n",
    "        \n",
    "        lidar_f = lidar_line(points,90,2)\n",
    "        lidar_r = lidar_line(points,45,2)\n",
    "        lidar_l = lidar_line(points,135,2)\n",
    "\n",
    "        if len(lidar_f) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            self.distance_to_obstacle_f = min(lidar_f[:,1])-2.247148275375366\n",
    "        \n",
    "        if len(lidar_r) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            self.distance_to_obstacle_r = np.sqrt(min(lidar_r[:,0]**2 + lidar_r[:,1]**2))\n",
    "        \n",
    "        if len(lidar_l) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            self.distance_to_obstacle_l = np.sqrt(min(lidar_l[:,0]**2 + lidar_l[:,1]**2))\n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        sleepy=0.1\n",
    "        if action == 0:\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, brake=0.0, steer = 0.3))\n",
    "            time.sleep(sleepy)\n",
    "            reward = 0.1\n",
    "        elif action == 1:\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, brake=0.0, steer = -0.3))\n",
    "            time.sleep(sleepy)\n",
    "            reward =0.1\n",
    "\n",
    "        \n",
    "        if len(self.collision_hist) != 0:\n",
    "            done = True\n",
    "            reward = -10\n",
    "        else :\n",
    "            done=False\n",
    "            reward=0.01\n",
    "            \n",
    "        if self.episode_start + SECONDS_PER_EPISODE < time.time():\n",
    "            done = True\n",
    "            \n",
    "        xx = self.distance_to_obstacle_f\n",
    "        yy = self.distance_to_obstacle_r\n",
    "        zz = self.distance_to_obstacle_l\n",
    "        state_=np.array([xx,yy,zz])\n",
    "            \n",
    "        return state_, reward, done, None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "\n",
    "    return (t1,t2,t3,t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "TRAIN\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 0) cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-d5fb2077d6d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TRAIN'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mcurrent_q_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQValues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_current\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-1371350de880>\u001b[0m in \u001b[0;36mextract_tensors\u001b[1;34m(experiences)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mt3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mt4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 0) cannot be concatenated"
     ]
    }
   ],
   "source": [
    "episode_durations = []\n",
    "env = CarEnv()\n",
    "xxx()\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = torch.from_numpy(state).float().to(device)\n",
    "    print(episode)\n",
    "    \n",
    "    for timestep in count():\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        memory.push(Experience(state, action[0], next_state, reward))\n",
    "        \n",
    "        state = torch.from_numpy(next_state).float().to(device)\n",
    "        \n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            print('TRAIN')\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "    \n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_states)\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(loss)\n",
    "        \n",
    "            \n",
    "        if done:\n",
    "            episode_durations.append(timestep)\n",
    "            #plot(episode_durations, 100)\n",
    "            for actor in env.actor_list:\n",
    "                actor.destroy()\n",
    "            break\n",
    "            \n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Experience(state=tensor([22.5353,  7.6613,  6.1497], device='cuda:0'), action=tensor([1], device='cuda:0'), next_state=array([22.53486276,  7.66132332,  6.1496472 ]), reward=0.01),\n",
       " Experience(state=tensor([-0.8140, 23.6337,  2.4432], device='cuda:0'), action=tensor([0], device='cuda:0'), next_state=array([-2.24196219,  1.57889511,  1.57889511]), reward=-10),\n",
       " Experience(state=tensor([22.5829,  7.6601,  6.1505], device='cuda:0'), action=tensor([1], device='cuda:0'), next_state=array([22.54787469,  7.66092366,  6.14993414]), reward=0.01),\n",
       " Experience(state=tensor([15.8860,  9.6127,  5.3174], device='cuda:0'), action=tensor([0], device='cuda:0'), next_state=array([ 5.15769887, 11.33385551,  4.4246896 ]), reward=0.01),\n",
       " Experience(state=tensor([22.6202,  7.6595,  6.1509], device='cuda:0'), action=tensor([1], device='cuda:0'), next_state=array([22.54787469,  7.66092366,  6.14993414]), reward=0.01),\n",
       " Experience(state=tensor([22.5374,  7.6611,  6.1498], device='cuda:0'), action=tensor([1], device='cuda:0'), next_state=array([22.53530526,  7.66126656,  6.14969306]), reward=0.01),\n",
       " Experience(state=tensor([20.8647,  8.1245,  5.8071], device='cuda:0'), action=tensor([1], device='cuda:0'), next_state=array([19.64630914,  6.9925882 ,  6.84419264]), reward=0.01),\n",
       " Experience(state=tensor([22.5374,  7.6611,  6.1498], device='cuda:0'), action=tensor([1], device='cuda:0'), next_state=array([22.53530526,  7.66126656,  6.14969306]), reward=0.01),\n",
       " Experience(state=tensor([ 7.7565,  5.5869, 12.1333], device='cuda:0'), action=tensor([0], device='cuda:0'), next_state=array([ 1.42385507,  3.87177659, 32.10555971]), reward=0.01),\n",
       " Experience(state=tensor([22.5353,  7.6613,  6.1497], device='cuda:0'), action=tensor([0], device='cuda:0'), next_state=array([22.53486276,  7.66132332,  6.1496472 ]), reward=0.01),\n",
       " Experience(state=tensor([19.6463,  6.9926,  6.8442], device='cuda:0'), action=tensor([0], device='cuda:0'), next_state=array([21.30527711,  7.52029706,  6.25685292]), reward=0.01),\n",
       " Experience(state=tensor([22.5349,  7.6613,  6.1496], device='cuda:0'), action=tensor([1], device='cuda:0'), next_state=array([22.40905213,  7.66677893,  6.14322914]), reward=0.01),\n",
       " Experience(state=tensor([22.5349,  7.6613,  6.1496], device='cuda:0'), action=tensor([1], device='cuda:0'), next_state=array([22.17046189,  7.84698241,  5.97453366]), reward=0.01),\n",
       " Experience(state=tensor([22.4091,  7.6668,  6.1432], device='cuda:0'), action=tensor([0], device='cuda:0'), next_state=array([20.86471391,  8.12451272,  5.80706102]), reward=0.01),\n",
       " Experience(state=tensor([ 1.4239,  3.8718, 32.1056], device='cuda:0'), action=tensor([1], device='cuda:0'), next_state=array([-0.74505973,  2.35258739,  2.78739512]), reward=-10),\n",
       " Experience(state=tensor([ 5.1577, 11.3339,  4.4247], device='cuda:0'), action=tensor([1], device='cuda:0'), next_state=array([ 5.14929032, 11.22856716,  3.67136828]), reward=0.01),\n",
       " Experience(state=tensor([22.5349,  7.6613,  6.1496], device='cuda:0'), action=tensor(0, device='cuda:0'), next_state=array([-0.81396079, 23.63368003,  2.44319206]), reward=0.01),\n",
       " Experience(state=tensor([22.5606,  7.6607,  6.1501], device='cuda:0'), action=tensor([1], device='cuda:0'), next_state=array([22.53740525,  7.66114742,  6.14977717]), reward=0.01),\n",
       " Experience(state=tensor([22.7166,  7.6587,  6.1514], device='cuda:0'), action=tensor(0, device='cuda:0'), next_state=array([22.56055474,  7.66070469,  6.15008157]), reward=0.01),\n",
       " Experience(state=tensor([22.1705,  7.8470,  5.9745], device='cuda:0'), action=tensor([1], device='cuda:0'), next_state=array([15.88603616,  9.61273341,  5.31736016]), reward=0.01)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = Experience(*zip(*experiences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cat() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-4afe0d6a2294>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: cat() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "torch.cat(0,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21.3053,  7.5203,  6.2569, 22.5606,  7.6607,  6.1501, 15.8860,  9.6127,\n",
       "         5.3174,  7.7565,  5.5869, 12.1333, 22.5353,  7.6613,  6.1497, 20.8647,\n",
       "         8.1245,  5.8071,  5.1577, 11.3339,  4.4247, 22.4091,  7.6668,  6.1432,\n",
       "        22.6202,  7.6595,  6.1509, 22.1705,  7.8470,  5.9745, 22.5829,  7.6601,\n",
       "         6.1505, 22.5374,  7.6611,  6.1498, 22.5353,  7.6613,  6.1497, 22.5374,\n",
       "         7.6611,  6.1498, -0.8140, 23.6337,  2.4432, 19.6463,  6.9926,  6.8442,\n",
       "        22.5349,  7.6613,  6.1496, 22.5353,  7.6613,  6.1497,  5.1493, 11.2286,\n",
       "         3.6714,  1.4239,  3.8718, 32.1056], device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(batch.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-03c64eb5d038>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got float"
     ]
    }
   ],
   "source": [
    "torch.cat(batch.reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01,\n",
       " 0.01,\n",
       " 0.01,\n",
       " 0.01,\n",
       " 0.01,\n",
       " 0.01,\n",
       " 0.01,\n",
       " 0.01,\n",
       " 0.01,\n",
       " 0.01,\n",
       " 0.01,\n",
       " 0.01,\n",
       " 0.01,\n",
       " 0.01,\n",
       " -10,\n",
       " 0.01,\n",
       " 0.01,\n",
       " 0.01,\n",
       " -10,\n",
       " -10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
